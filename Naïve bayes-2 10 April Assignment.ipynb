{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d615482-327a-4d16-9e29-0bb30f49f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. You can use the formula for conditional probability:\n",
    "\n",
    "P(A | B) = P(A and B) / P(B)\n",
    "\n",
    "In this case:\n",
    "\n",
    "A represents the event \"employee is a smoker.\"\n",
    "B represents the event \"employee uses the health insurance plan.\"\n",
    "You are given the following probabilities:\n",
    "\n",
    "P(B) = Probability that an employee uses the health insurance plan = 70% = 0.70\n",
    "P(A | B) = Probability that an employee is a smoker given that he/she uses the health insurance plan (what we want to find).\n",
    "You are also given that 40% of the employees who use the plan are smokers, which means:\n",
    "\n",
    "P(A and B) = Probability that an employee is a smoker and uses the health insurance plan = 40% = 0.40\n",
    "Now you can use the formula to find P(A | B):\n",
    "\n",
    "P(A | B) = P(A and B) / P(B) = 0.40 / 0.70 â‰ˆ 0.5714\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm used in machine learning for classification tasks, but they are designed for different types of data and have some key differences:\n",
    "\n",
    "Data Representation:\n",
    "\n",
    "Bernoulli Naive Bayes: It is typically used when dealing with binary or Boolean data, where features are either present or absent, such as text classification where each word is treated as a binary feature (whether a word occurs in a document or not).\n",
    "\n",
    "Multinomial Naive Bayes: It is used when dealing with discrete data, often in the form of frequency counts. It is commonly used for text classification tasks where the features represent word counts or term frequencies in documents.\n",
    "\n",
    "Feature Distribution:\n",
    "\n",
    "Bernoulli Naive Bayes: Assumes that features are binary variables and models the presence or absence of each feature independently for each class. It is suitable for problems where the presence or absence of features is important, such as spam detection.\n",
    "\n",
    "Multinomial Naive Bayes: Assumes that features follow a multinomial distribution, which is suitable for problems where the frequency or count of features is important, such as document classification based on word counts.\n",
    "\n",
    "Probability Estimation:\n",
    "\n",
    "Bernoulli Naive Bayes: Uses the Bernoulli distribution to estimate probabilities. It calculates the likelihood of observing each feature for each class and assumes that the features are conditionally independent given the class.\n",
    "\n",
    "Multinomial Naive Bayes: Uses the multinomial distribution to estimate probabilities. It calculates the likelihood of observing feature counts for each class and also assumes conditional independence of features.\n",
    "\n",
    "Example Use Cases:\n",
    "\n",
    "Bernoulli Naive Bayes is often used for problems like sentiment analysis, where you want to classify documents as positive or negative based on the presence or absence of specific words or features.\n",
    "\n",
    "Multinomial Naive Bayes is commonly used for tasks like document categorization, spam email detection, and text classification, where the frequency of words or terms in documents is important.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "\n",
    "Bernoulli Naive Bayes, like other Naive Bayes variants, assumes that features are conditionally independent given the class label. When dealing with missing values in a Bernoulli Naive Bayes model, there are several common approaches:\n",
    "\n",
    "Imputation: One common approach is to impute missing values with a default value that represents \"unknown\" or \"missing.\" For a Bernoulli Naive Bayes model, this default value is often set to 0 (absence of the feature). This assumes that if a feature is missing, it is treated as if it were not present. This imputation allows you to include instances with missing values in your classification process.\n",
    "\n",
    "Deletion: Another option is to simply remove instances with missing values from the dataset. This is a straightforward approach but can lead to a loss of data and potentially biased results, especially if missing values are not missing completely at random (MCAR).\n",
    "\n",
    "Model-Based Imputation: You can also use more sophisticated techniques, such as model-based imputation. This involves using the information from the other features and the class labels to estimate the missing values. For Bernoulli Naive Bayes, you might estimate the missing feature values based on the conditional probabilities of observing the feature given the class label.\n",
    "\n",
    "Binary Indicator Variable: Another approach is to introduce an additional binary indicator variable for each feature that indicates whether the original feature was missing or not. This way, you explicitly model the missingness as a separate feature. You can then train the Bernoulli Naive Bayes model with these additional indicator features.\n",
    "\n",
    "Use of Bayesian Networks: In some cases, Bayesian networks or more complex probabilistic graphical models can be used to handle missing values more effectively. These models can capture dependencies among variables and provide better imputation strategies.\n",
    "\n",
    "The choice of how to handle missing values in a Bernoulli Naive Bayes model depends on the specific characteristics of your dataset and the problem you are trying to solve. Imputation with a default value or using indicator variables are common and simple approaches, but more advanced techniques may be appropriate in certain situations, especially when missing data patterns are complex. It's important to consider the potential impact of missing values on your classification results and choose an approach that aligns with your modeling goals and assumptions.\n",
    "\n",
    "\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that the features in the dataset follow a Gaussian (normal) distribution. It is particularly well-suited for continuous data where each feature is modeled as a Gaussian distribution for each class.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes works by estimating the parameters (mean and variance) of the Gaussian distribution for each feature within each class. When making predictions for a new instance, the algorithm calculates the likelihood of the instance's feature values under each class's Gaussian distribution and combines this with prior class probabilities to determine the most likely class.\n",
    "\n",
    "Here's a high-level overview of how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "Parameter Estimation: For each class in the training dataset, Gaussian Naive Bayes estimates the mean and variance of each feature's distribution. These parameters are used to describe the Gaussian distribution for each feature within each class.\n",
    "\n",
    "Prior Probabilities: It also calculates the prior probabilities of each class, which represent the likelihood of each class occurring in the dataset.\n",
    "\n",
    "Predictions: When making predictions for a new instance with feature values, Gaussian Naive Bayes calculates the likelihood of the feature values under the Gaussian distribution of each class and combines this likelihood with the prior probabilities using Bayes' theorem to compute the posterior probabilities for each class.\n",
    "\n",
    "Class Assignment: The class with the highest posterior probability is chosen as the predicted class for the new instance.\n",
    "\n",
    "So, Gaussian Naive Bayes can handle multi-class classification by estimating the parameters for each class and selecting the class that has the highest probability for a given instance. It's a simple and efficient algorithm, but it makes the assumption that the features are continuous and follow a Gaussian distribution, which may not always be the case in practice. If this assumption doesn't hold, other variants of Naive Bayes or different classification algorithms may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Assignment:\n",
    "\n",
    "    \n",
    "    \n",
    "    Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data Preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
    "\n",
    "You can use the provided URL: https://archive.ics.uci.edu/ml/datasets/Spambase.\n",
    "Download the dataset and save it to your local machine.\n",
    "Load the dataset into a pandas DataFrame or using any other suitable method.\n",
    "\n",
    "Preprocess the data as necessary. This may involve handling missing values, scaling/normalizing features, and splitting the dataset into training and testing sets.\n",
    "\n",
    "Implementation:\n",
    "Import the necessary libraries, including scikit-learn and pandas.\n",
    "\n",
    "Split the dataset into features (X) and target labels (y). The target labels should indicate whether an email is spam or not (1 for spam, 0 for non-spam).\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn.\n",
    "\n",
    "Perform 10-fold cross-validation for each classifier using the cross_val_score function from scikit-learn. Calculate accuracy, precision, recall, and F1-score for each fold.\n",
    "\n",
    "Results:\n",
    "For each classifier, calculate the mean and standard deviation of accuracy, precision, recall, and F1-score across the 10 folds. You can use numpy to compute these statistics.\n",
    "\n",
    "Report the performance metrics (accuracy, precision, recall, F1-score) for each classifier.\n",
    "\n",
    "Discussion:\n",
    "Analyze and discuss the results. Compare the performance of Bernoulli, Multinomial, and Gaussian Naive Bayes classifiers.\n",
    "\n",
    "Explain why one variant of Naive Bayes might have performed better than the others based on the characteristics of the dataset and the assumptions of each classifier.\n",
    "\n",
    "Discuss any limitations or challenges you observed during the analysis, such as the impact of feature distribution on classifier performance.\n",
    "\n",
    "Conclusion:\n",
    "Summarize your findings and provide suggestions for future work. What could be done to improve the performance of the classifiers, or are there other algorithms that might be more suitable for this dataset?\n",
    "\n",
    "Share your Jupyter Notebook containing the code, results, and discussion on a public GitHub repository. Ensure that the repository is well-documented and organized.\n",
    "\n",
    "Share the GitHub repository link through your assignment submission on your dashboard.\n",
    "\n",
    "By following these steps, you should be able to complete the assignment and provide a comprehensive analysis of the performance of different Naive Bayes classifiers on the Spambase dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
